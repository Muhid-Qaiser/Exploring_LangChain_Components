{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Muhid Qaiser \n",
    "\n",
    "Email : muhidqaiser02@gmail.com \n",
    "\n",
    "Linkedin : https://www.linkedin.com/in/muhid-qaiser/\n",
    "\n",
    "Github : https://github.com/Muhid-Qaiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion - Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .txt File Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../Documnets/Global_Warming_Pak.txt'}, page_content='Global Warming in Pakistan: A Looming Crisis\\n\\nIntroduction\\n\\nGlobal warming is one of the most pressing environmental challenges of the 21st century, and Pakistan is among the countries most vulnerable to its effects. Rising temperatures, melting glaciers, changing precipitation patterns, and increasing frequency of extreme weather events are severely impacting Pakistanâ€™s ecosystem, economy, and society. This essay explores the causes, consequences, and potential solutions to global warming in Pakistan.\\n\\nCauses of Global Warming in Pakistan\\n\\nThe primary driver of global warming in Pakistan, as elsewhere, is the excessive emission of greenhouse gases (GHGs) such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). These emissions result from industrial activities, deforestation, and the burning of fossil fuels for energy production and transportation. In Pakistan, the major contributors to greenhouse gas emissions include:\\n\\nDeforestation: Pakistan has one of the highest deforestation rates in the world. Large-scale cutting of trees for fuel, agriculture, and urban expansion has significantly reduced the countryâ€™s forest cover, thereby decreasing the natural absorption of CO2.\\n\\nEnergy Sector: The reliance on coal, oil, and gas for electricity generation and transportation contributes significantly to carbon emissions. Despite efforts to promote renewable energy, fossil fuels still dominate Pakistanâ€™s energy mix.\\n\\nAgricultural Practices: Unsustainable agricultural methods, including excessive use of chemical fertilizers and inefficient irrigation techniques, contribute to methane and nitrous oxide emissions.\\n\\nIndustrialization: Rapid industrialization without adequate environmental regulations leads to unchecked emissions of pollutants that contribute to global warming.\\n\\nConsequences of Global Warming in Pakistan\\n\\nThe impact of global warming in Pakistan is already evident in multiple ways:\\n\\nRising Temperatures: Pakistan has experienced record-breaking heatwaves in recent years. The temperature in some regions, such as Jacobabad and Sibi, has exceeded 50Â°C, making them among the hottest places on Earth.\\n\\nGlacial Melting: The countryâ€™s glaciers, particularly in the northern regions, are melting at an alarming rate due to rising temperatures. This leads to glacial lake outburst floods (GLOFs), endangering nearby communities.\\n\\nWater Scarcity: Changes in precipitation patterns and glacial melt are disrupting the water supply in Pakistan. The Indus River, the countryâ€™s primary water source, is becoming increasingly unpredictable, posing a threat to agriculture and drinking water availability.\\n\\nExtreme Weather Events: Pakistan has witnessed a surge in extreme weather events, including floods, droughts, and storms. The devastating floods of 2022, which submerged one-third of the country and displaced millions, were exacerbated by climate change.\\n\\nAgricultural Decline: Climate change is negatively affecting crop yields, leading to food insecurity and economic instability. The unpredictability of monsoons and the increased frequency of droughts have made farming increasingly difficult.\\n\\nHealth Risks: The rise in temperatures and air pollution has led to an increase in heat-related illnesses, respiratory diseases, and vector-borne diseases such as dengue and malaria.\\n\\nSolutions to Global Warming in Pakistan\\n\\nWhile Pakistan faces severe climate challenges, several measures can be taken to mitigate the impact of global warming:\\n\\nAfforestation and Reforestation: Initiatives such as the Billion Tree Tsunami project should be expanded to increase forest cover and enhance carbon sequestration.\\n\\nRenewable Energy Transition: Investing in solar, wind, and hydropower can reduce reliance on fossil fuels and decrease greenhouse gas emissions.\\n\\nSustainable Agriculture: The adoption of climate-smart agricultural practices, such as efficient irrigation and organic farming, can help reduce methane and nitrous oxide emissions.\\n\\nImproved Water Management: Constructing more reservoirs, improving irrigation efficiency, and promoting water conservation can address water scarcity issues.\\n\\nDisaster Preparedness and Adaptation: Strengthening early warning systems, building climate-resilient infrastructure, and implementing effective disaster management strategies can mitigate the impact of extreme weather events.\\n\\nInternational Cooperation: Pakistan should actively participate in global climate initiatives, seek climate financing, and implement policies aligned with international climate agreements, such as the Paris Agreement.\\n\\nConclusion\\n\\nGlobal warming is a critical threat to Pakistan, with far-reaching consequences for its environment, economy, and people. Addressing this crisis requires a multi-faceted approach that includes afforestation, renewable energy adoption, sustainable agriculture, and improved disaster preparedness. While the government has initiated several programs to combat climate change, more concerted efforts are needed at the national and international levels to secure a sustainable future for Pakistan. Only through collective action can the country mitigate the effects of global warming and build resilience against future climate challenges.\\n\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../Documnets/Global_Warming_Pak.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .pdf File Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 0}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)Codeforces\\n(Percentile)GPQA Diamond\\n(Pass@1)MATH-500\\n(Pass@1)MMLU\\n(Pass@1)SWE-bench Verified\\n(Resolved)020406080100Accuracy / Percentile (%)79.896.3\\n71.597.3\\n90.8\\n49.279.296.6\\n75.796.4\\n91.8\\n48.972.690.6\\n62.194.3\\n87.4\\n36.863.693.4\\n60.090.0\\n85.2\\n41.6\\n39.258.7 59.190.2\\n88.5\\n42.0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\\nFigure 1|Benchmark performance of DeepSeek-R1.arXiv:2501.12948v1  [cs.CL]  22 Jan 2025'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 1}, page_content='Contents\\n1 Introduction 3\\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Approach 5\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5\\n2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9\\n2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10\\n2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10\\n2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11\\n2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11\\n3 Experiment 11\\n3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Discussion 14\\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion, Limitations, and Future Work 16\\nA Contributions and Acknowledgments 20\\n2'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 2}, page_content='1. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\\ntowards Artificial General Intelligence (AGI).\\nRecently, post-training has emerged as an important component of the full training pipeline.\\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\\nof OpenAI-o1-0912.\\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 3}, page_content='1.1. Contributions\\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\\n•We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurely through RL, without the need for SFT. This breakthrough paves the way for future\\nadvancements in this area.\\n•We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\\nerences, as well as two SFT stages that serve as the seed for the model’s reasoning and\\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\\nbetter models.\\nDistillation: Smaller Models Can Be Powerful Too\\n•We demonstrate that the reasoning patterns of larger models can be distilled into smaller\\nmodels, resulting in better performance compared to the reasoning patterns discovered\\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit\\nthe research community to distill better smaller models in the future.\\n•Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\\nthat are widely used in the research community. The evaluation results demonstrate that\\nthe distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-\\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n•Reasoning tasks : (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n•Knowledge : On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 4}, page_content='•Others : DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data ,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1,𝑜2,···,𝑜𝐺}from the old\\npolicy𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺\\n𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\\n1\\n𝐺𝐺∑︁\\n𝑖=1\\x12\\nmin\\x12𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖, clip\\x12𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞), 1−𝜀, 1+𝜀\\x13\\n𝐴𝑖\\x13\\n−𝛽D𝐾𝐿\\x00\\n𝜋𝜃||𝜋𝑟𝑒𝑓\\x01\\x13\\n,(1)\\nD𝐾𝐿\\x00\\n𝜋𝜃||𝜋𝑟𝑒𝑓\\x01=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞)−1, (2)\\nwhere𝜀and𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of\\nrewards{𝑟1,𝑟2,...,𝑟𝐺}corresponding to the outputs within each group:\\n𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\\ns𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\\n5'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 5}, page_content='A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\nThe assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>. User: prompt. Assistant:\\nTable 1|Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\\nquestion during training.\\n2.2.2. Reward Modeling\\nThe reward is the source of the training signal, which decides the optimization direction of RL.\\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\\ntypes of rewards:\\n•Accuracy rewards : The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n•Format rewards : In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training Template\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 6}, page_content='ModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\\nTable 2|Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2|AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 7}, page_content='Figure 3|The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 8}, page_content='Question: If 𝑎 >1, then the sum of the real solutions of√︁\\n𝑎−√\\n𝑎+𝑥=𝑥is equal to\\nResponse: <think>\\nTo solve the equation√︁\\n𝑎−√\\n𝑎+𝑥=𝑥, let’s start by squaring both ···\\x10√︁\\n𝑎−√\\n𝑎+𝑥\\x112\\n=𝑥2=⇒𝑎−√\\n𝑎+𝑥=𝑥2.\\nRearrange to isolate the inner square root term:\\n(𝑎−𝑥2)2=𝑎+𝑥=⇒𝑎2−2𝑎𝑥2+(𝑥2)2=𝑎+𝑥=⇒𝑥4−2𝑎𝑥2−𝑥+(𝑎2−𝑎)=0\\n. . .\\nWait, wait. Wait. That’s an aha moment I can flag here.\\nLet’s reevaluate this step-by-step to identify if the correct sum can be ···\\nWe started with the equation:√︁\\n𝑎−√\\n𝑎+𝑥=𝑥\\nFirst, let’s square both sides:\\n𝑎−√\\n𝑎+𝑥=𝑥2=⇒√\\n𝑎+𝑥=𝑎−𝑥2\\nNext, I could square both sides again, treating the equation: ···\\n. . .\\nTable 3|An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The\\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\\nallowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.\\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 9}, page_content='include:\\n•Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\\nwe design a readable pattern that includes a summary at the end of each response and\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\\nresults.\\n•Potential: By carefully designing the pattern for cold-start data with human priors, we\\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\\na better way for reasoning models.\\n2.3.2. Reasoning-oriented Reinforcement Learning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\\non enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such\\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language\\nmixing, we introduce a language consistency reward during RL training, which is calculated\\nas the proportion of target language words in the CoT. Although ablation experiments show\\nthat such alignment results in a slight degradation in the model’s performance, this reward\\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\\nreasoning tasks and the reward for language consistency by directly summing them to form the\\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\\non reasoning tasks.\\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\\nmodel’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.\\nReasoning data We curate reasoning prompts and generate reasoning trajectories by perform-\\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\\nwe expand the dataset by incorporating additional data, some of which use a generative reward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\\nabout 600k reasoning related training samples.\\n10'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 10}, page_content='Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,\\nand translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thought before answering the question by prompting. However, for simpler queries,\\nsuch as “hello” we do not provide a CoT in response. In the end, we collected a total of\\napproximately 200k training samples that are unrelated to reasoning.\\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\\n800k samples.\\n2.3.4. Reinforcement Learning for all Scenarios\\nTo further align the model with human preferences, we implement a secondary reinforcement\\nlearning stage aimed at improving the model’s helpfulness and harmlessness while simultane-\\nously refining its reasoning capabilities. Specifically, we train the model using a combination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\\nresponse of the model, including both the reasoning process and the summary, to identify and\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\\n2.4. Distillation: Empower Small Models with Reasoning Capability\\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that\\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\\nreasoning capability is slightly better than that of Llama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstage to the broader research community.\\n3. Experiment\\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\\n11'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 11}, page_content='2024d), Aider1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces2, Chinese\\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\\nematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we\\nalso evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we\\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\\nevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\\nzero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\\nC#, JavaScript, TypeScript, PHP , and Bash). Model performance on LiveCodeBench is evaluated\\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\\nof 32,768 tokens for each benchmark.\\nBaselines We conduct comprehensive evaluations against several strong baselines, including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@ 𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6and a top- 𝑝value of 0.95 to generate 𝑘\\nresponses (typically between 4and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\npass@1 =1\\n𝑘𝑘∑︁\\n𝑖=1𝑝𝑖,\\nwhere𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n12'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 12}, page_content='3.1. DeepSeek-R1 Evaluation\\nBenchmark (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\\nArchitecture - - MoE - - MoE\\n# Activated Params - - 37B - - 37B\\n# Total Params - - 671B - - 671B\\nEnglishMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\\nCodeLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\\nCodeforces (Rating) 717 759 1134 1820 2061 2029\\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\\nMathAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\\nChineseCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\\nTable 4|Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 13}, page_content='DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\\nits robustness across multiple tasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntraining data currently remains very limited.\\n3.2. Distilled Model Evaluation\\nModelAIME 2024 MATH-500GPQA LiveCodeCodeForcesDiamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\\nDeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954\\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\\nDeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\\nDeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633\\nTable 5|Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\nAs shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v.s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?\\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\\n14'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 14}, page_content='ModelAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable 6|Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\\nscale reinforcement learning.\\n4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM) PRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\nMonte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\nand the value model, iteratively refining the process.\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n15'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 15}, page_content='exponentially larger search space. To address this, we set a maximum extension limit for each\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\nmodel to iteratively improve. While AlphaGo’s core success relied on training a value model to\\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\\ndue to the complexities of token generation.\\nIn conclusion, while MCTS can improve performance during inference when paired with a\\npre-trained value model, iteratively boosting model performance through self-search remains a\\nsignificant challenge.\\n5. Conclusion, Limitations, and Future Work\\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntuned models based on the same underlying checkpoints.\\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\\n•General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\\nthese fields.\\n•Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\\nin a language other than English or Chinese. We aim to address this limitation in future\\nupdates.\\n•Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive\\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shot setting for optimal results.\\n•Software Engineering Tasks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 16}, page_content='References\\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md .\\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\\n-5-sonnet .\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,\\nF. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374 .\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\nY. Dubois, B. Galambosi, P . Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179 .\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL\\nhttps://arxiv.org/abs/2210.10760 .\\nA. P . Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP . Minervini. Are we done with mmlu? CoRR , abs/2406.04127, 2024. URL https://doi.or\\ng/10.48550/arXiv.2406.04127 .\\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024 .\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974 .\\n17'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 17}, page_content='S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR ,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941 .\\nA. Kumar, V . Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212 ,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.\\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\\nhttps://github.com/WildEval/ZeroEval .\\nMAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination -AIME 2024 , February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime .\\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/ .\\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\\ng-to-reason-with-llms/ .\\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\\n-simpleqa/ .\\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\\n-verified/ .\\nQwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm\\n.github.io/blog/qwq-32b-preview/ .\\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b\\nlog/qwen2.5 .\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\\nGPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022 , 2023.\\nZ. Shao, P . Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD. Kumaran, T. Graepel, T. P . Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and\\nshogi by self-play with a general reinforcement learning algorithm. CoRR , abs/1712.01815,\\n2017a. URL http://arxiv.org/abs/1712.01815 .\\n18'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 18}, page_content='D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\\nM. Lai, A. Bolton, Y. Chen, T. P . Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and\\nD. Hassabis. Mastering the game of go without human knowledge. Nat. , 550(7676):354–359,\\n2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270 .\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033\\n14.\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv\\npreprint arXiv:2211.14275, 2022.\\nP . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-\\nfree step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935 ,\\n2023.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022.\\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR , abs/2406.01574, 2024.\\nURL https://doi.org/10.48550/arXiv.2406.01574 .\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software\\nengineering agents. arXiv preprint, 2024.\\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,\\nQ. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing\\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL\\nhttps://arxiv.org/abs/2408.08152 .\\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\\nevaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\\n19'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 19}, page_content='Appendix\\nA. Contributions and Acknowledgments\\nCore Contributors\\nDaya Guo\\nDejian Yang\\nHaowei Zhang\\nJunxiao Song\\nRuoyu Zhang\\nRunxin Xu\\nQihao Zhu\\nShirong Ma\\nPeiyi Wang\\nXiao Bi\\nXiaokang Zhang\\nXingkai Yu\\nYu Wu\\nZ.F. Wu\\nZhibin Gou\\nZhihong Shao\\nZhuoshu Li\\nZiyi Gao\\nContributors\\nAixin Liu\\nBing Xue\\nBingxuan Wang\\nBochao Wu\\nBei Feng\\nChengda Lu\\nChenggang Zhao\\nChengqi Deng\\nChong Ruan\\nDamai Dai\\nDeli Chen\\nDongjie Ji\\nErhang Li\\nFangyun Lin\\nFucong Dai\\nFuli Luo*\\nGuangbo Hao\\nGuanting Chen\\nGuowei Li\\nH. Zhang\\nHanwei Xu\\nHonghui Ding\\nHuazuo Gao\\nHui QuHui Li\\nJianzhong Guo\\nJiashi Li\\nJingchang Chen\\nJingyang Yuan\\nJinhao Tu\\nJunjie Qiu\\nJunlong Li\\nJ.L. Cai\\nJiaqi Ni\\nJian Liang\\nJin Chen\\nKai Dong\\nKai Hu*\\nKaichao You\\nKaige Gao\\nKang Guan\\nKexin Huang\\nKuai Yu\\nLean Wang\\nLecong Zhang\\nLiang Zhao\\nLitong Wang\\nLiyue Zhang\\nLei Xu\\nLeyi Xia\\nMingchuan Zhang\\nMinghua Zhang\\nMinghui Tang\\nMingxu Zhou\\nMeng Li\\nMiaojun Wang\\nMingming Li\\nNing Tian\\nPanpan Huang\\nPeng Zhang\\nQiancheng Wang\\nQinyu Chen\\nQiushi Du\\nRuiqi Ge*\\nRuisong Zhang\\nRuizhe Pan\\nRunji Wang\\nR.J. Chen\\nR.L. Jin\\n20'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 20}, page_content='Ruyi Chen\\nShanghao Lu\\nShangyan Zhou\\nShanhuang Chen\\nShengfeng Ye\\nShiyu Wang\\nShuiping Yu\\nShunfeng Zhou\\nShuting Pan\\nS.S. Li\\nShuang Zhou\\nShaoqing Wu\\nShengfeng Ye\\nTao Yun\\nTian Pei\\nTianyu Sun\\nT. Wang\\nWangding Zeng\\nWen Liu\\nWenfeng Liang\\nWenjun Gao\\nWenqin Yu*\\nWentao Zhang\\nW.L. Xiao\\nWei An\\nXiaodong Liu\\nXiaohan Wang\\nXiaokang Chen\\nXiaotao Nie\\nXin Cheng\\nXin Liu\\nXin Xie\\nXingchao Liu\\nXinyu Yang\\nXinyuan Li\\nXuecheng Su\\nXuheng Lin\\nX.Q. Li\\nXiangyue Jin\\nXiaojin Shen\\nXiaosha Chen\\nXiaowen Sun\\nXiaoxiang Wang\\nXinnan Song\\nXinyi Zhou\\nXianzu Wang\\nXinxia Shan\\nY.K. Li\\nY.Q. WangY.X. Wei\\nYang Zhang\\nYanhong Xu\\nYao Li\\nYao Zhao\\nYaofeng Sun\\nYaohui Wang\\nYi Yu\\nYichao Zhang\\nYifan Shi\\nYiliang Xiong\\nYing He\\nYishi Piao\\nYisong Wang\\nYixuan Tan\\nYiyang Ma*\\nYiyuan Liu\\nYongqiang Guo\\nYuan Ou\\nYuduan Wang\\nYue Gong\\nYuheng Zou\\nYujia He\\nYunfan Xiong\\nYuxiang Luo\\nYuxiang You\\nYuxuan Liu\\nYuyang Zhou\\nY.X. Zhu\\nYanping Huang\\nYaohui Li\\nYi Zheng\\nYuchen Zhu\\nYunxian Ma\\nYing Tang\\nYukun Zha\\nYuting Yan\\nZ.Z. Ren\\nZehui Ren\\nZhangli Sha\\nZhe Fu\\nZhean Xu\\nZhenda Xie\\nZhengyan Zhang\\nZhewen Hao\\nZhicheng Ma\\nZhigang Yan\\nZhiyu Wu\\nZihui Gu\\n21'),\n",
       " Document(metadata={'source': 'Documnets/deepseek_r1.pdf', 'page': 21}, page_content='Zijia Zhu\\nZijun Liu*\\nZilin Li\\nZiwei Xie\\nZiyang Song\\nZizheng PanZhen Huang\\nZhipeng Xu\\nZhongyu Zhang\\nZhen Zhang\\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\\ndenote individuals who have departed from our team.\\n22')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../Documnets/deepseek_r1.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page converted into a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .csv File Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 0}, page_content='Year: 2013\\nTemperature Increase (Â°C): 0.8\\nCO2 Emissions (Million Tons): 180\\nDeforestation Rate (%): 1.2\\nGlacial Melt Rate (%): 0.5\\nWater Scarcity Index: 0.6\\nFlood Events: 2\\nDrought Events: 1\\nAgricultural Yield Change (%): -2\\nRenewable Energy Share (%): 10'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 1}, page_content='Year: 2014\\nTemperature Increase (Â°C): 0.9\\nCO2 Emissions (Million Tons): 185\\nDeforestation Rate (%): 1.3\\nGlacial Melt Rate (%): 0.6\\nWater Scarcity Index: 0.61\\nFlood Events: 3\\nDrought Events: 1\\nAgricultural Yield Change (%): -2.5\\nRenewable Energy Share (%): 11'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 2}, page_content='Year: 2015\\nTemperature Increase (Â°C): 1.0\\nCO2 Emissions (Million Tons): 190\\nDeforestation Rate (%): 1.4\\nGlacial Melt Rate (%): 0.7\\nWater Scarcity Index: 0.62\\nFlood Events: 4\\nDrought Events: 2\\nAgricultural Yield Change (%): -3\\nRenewable Energy Share (%): 12'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 3}, page_content='Year: 2016\\nTemperature Increase (Â°C): 1.1\\nCO2 Emissions (Million Tons): 195\\nDeforestation Rate (%): 1.5\\nGlacial Melt Rate (%): 0.8\\nWater Scarcity Index: 0.64\\nFlood Events: 2\\nDrought Events: 2\\nAgricultural Yield Change (%): -3.5\\nRenewable Energy Share (%): 13'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 4}, page_content='Year: 2017\\nTemperature Increase (Â°C): 1.2\\nCO2 Emissions (Million Tons): 200\\nDeforestation Rate (%): 1.6\\nGlacial Melt Rate (%): 0.9\\nWater Scarcity Index: 0.65\\nFlood Events: 5\\nDrought Events: 3\\nAgricultural Yield Change (%): -4\\nRenewable Energy Share (%): 14'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 5}, page_content='Year: 2018\\nTemperature Increase (Â°C): 1.3\\nCO2 Emissions (Million Tons): 205\\nDeforestation Rate (%): 1.7\\nGlacial Melt Rate (%): 1.0\\nWater Scarcity Index: 0.67\\nFlood Events: 6\\nDrought Events: 3\\nAgricultural Yield Change (%): -4.5\\nRenewable Energy Share (%): 15'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 6}, page_content='Year: 2019\\nTemperature Increase (Â°C): 1.4\\nCO2 Emissions (Million Tons): 210\\nDeforestation Rate (%): 1.8\\nGlacial Melt Rate (%): 1.1\\nWater Scarcity Index: 0.68\\nFlood Events: 7\\nDrought Events: 4\\nAgricultural Yield Change (%): -5\\nRenewable Energy Share (%): 16'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 7}, page_content='Year: 2020\\nTemperature Increase (Â°C): 1.5\\nCO2 Emissions (Million Tons): 215\\nDeforestation Rate (%): 1.9\\nGlacial Melt Rate (%): 1.2\\nWater Scarcity Index: 0.70\\nFlood Events: 8\\nDrought Events: 4\\nAgricultural Yield Change (%): -5.5\\nRenewable Energy Share (%): 17'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 8}, page_content='Year: 2021\\nTemperature Increase (Â°C): 1.6\\nCO2 Emissions (Million Tons): 220\\nDeforestation Rate (%): 2.0\\nGlacial Melt Rate (%): 1.3\\nWater Scarcity Index: 0.72\\nFlood Events: 9\\nDrought Events: 5\\nAgricultural Yield Change (%): -6\\nRenewable Energy Share (%): 18'),\n",
       " Document(metadata={'source': 'Documnets/Global_Warming.csv', 'row': 9}, page_content='Year: 2022\\nTemperature Increase (Â°C): 1.7\\nCO2 Emissions (Million Tons): 225\\nDeforestation Rate (%): 2.1\\nGlacial Melt Rate (%): 1.4\\nWater Scarcity Index: 0.74\\nFlood Events: 10\\nDrought Events: 5\\nAgricultural Yield Change (%): -6.5\\nRenewable Energy Share (%): 19')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path='../Documnets/Global_Warming.csv')\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Row converted into a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-01-22', 'Title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'Authors': 'DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang', 'Summary': 'We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\\nHowever, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we\\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\\non reasoning tasks. To support the research community, we open-source\\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.'}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nGPQA Diamond\\n(Pass@1)\\nMATH-500\\n(Pass@1)\\nMMLU\\n(Pass@1)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy / Percentile (%)\\n79.8\\n96.3\\n71.5\\n97.3\\n90.8\\n49.2\\n79.2\\n96.6\\n75.7\\n96.4\\n91.8\\n48.9\\n72.6\\n90.6\\n62.1\\n94.3\\n87.4\\n36.8\\n63.6\\n93.4\\n60.0\\n90.0\\n85.2\\n41.6\\n39.2\\n58.7\\n59.1\\n90.2\\n88.5\\n42.0\\nDeepSeek-R1\\nOpenAI-o1-1217\\nDeepSeek-R1-32B\\nOpenAI-o1-mini\\nDeepSeek-V3\\nFigure 1 | Benchmark performance of DeepSeek-R1.\\narXiv:2501.12948v1  [cs.CL]  22 Jan 2025\\nContents\\n1\\nIntroduction\\n3\\n1.1\\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n1.2\\nSummary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2\\nApproach\\n5\\n2.1\\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\nDeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .\\n5\\n2.2.1\\nReinforcement Learning Algorithm\\n. . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2.2\\nReward Modeling\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.3\\nTraining Template\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.4\\nPerformance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\n6\\n2.3\\nDeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .\\n9\\n2.3.1\\nCold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.3.2\\nReasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .\\n10\\n2.3.3\\nRejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .\\n10\\n2.3.4\\nReinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . .\\n11\\n2.4\\nDistillation: Empower Small Models with Reasoning Capability . . . . . . . . . .\\n11\\n3\\nExperiment\\n11\\n3.1\\nDeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n3.2\\nDistilled Model Evaluation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4\\nDiscussion\\n14\\n4.1\\nDistillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4.2\\nUnsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n5\\nConclusion, Limitations, and Future Work\\n16\\nA Contributions and Acknowledgments\\n20\\n2\\n1. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\\ntowards Artificial General Intelligence (AGI).\\nRecently, post-training has emerged as an important component of the full training pipeline.\\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\\nof OpenAI-o1-0912.\\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3\\n1.1. Contributions\\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\\n• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurely through RL, without the need for SFT. This breakthrough paves the way for future\\nadvancements in this area.\\n• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\\nerences, as well as two SFT stages that serve as the seed for the model’s reasoning and\\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\\nbetter models.\\nDistillation: Smaller Models Can Be Powerful Too\\n• We demonstrate that the reasoning patterns of larger models can be distilled into smaller\\nmodels, resulting in better performance compared to the reasoning patterns discovered\\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit\\nthe research community to distill better smaller models in the future.\\n• Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\\nthat are widely used in the research community. The evaluation results demonstrate that\\nthe distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-\\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization\\nIn order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old\\npolicy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\\n𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\\n1\\n𝐺\\n𝐺\\n∑︁\\n𝑖=1\\n\\x12\\nmin\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀\\n\\x13\\n𝐴𝑖\\n\\x13\\n−𝛽D𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01\\x13\\n,\\n(1)\\nD𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01 =\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −log\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −1,\\n(2)\\nwhere 𝜀and 𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of\\nrewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:\\n𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\ns𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\n.\\n(3)\\n5\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\nThe assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>. User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\\nquestion during training.\\n2.2.2. Reward Modeling\\nThe reward is the source of the training signal, which decides the optimization direction of RL.\\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\\ntypes of rewards:\\n• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n• Format rewards: In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training Template\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero\\nFigure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nOpenAI-o1-0912\\n74.4\\n83.3\\n94.8\\n77.3\\n63.4\\n1843\\nDeepSeek-R1-Zero\\n71.0\\n86.7\\n95.9\\n73.3\\n50.0\\n1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero\\nThe self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7\\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero\\nA particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8\\nQuestion: If 𝑎> 1, then the sum of the real solutions of\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥is equal to\\nResponse: <think>\\nTo solve the equation\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥, let’s start by squaring both · · ·\\n\\x10√︁\\n𝑎−√\\n𝑎+ 𝑥\\n\\x112\\n= 𝑥2 =⇒𝑎−√\\n𝑎+ 𝑥= 𝑥2.\\nRearrange to isolate the inner square root term:\\n(𝑎−𝑥2)2 = 𝑎+ 𝑥=⇒𝑎2 −2𝑎𝑥2 + (𝑥2)2 = 𝑎+ 𝑥=⇒𝑥4 −2𝑎𝑥2 −𝑥+ (𝑎2 −𝑎) = 0\\n. . .\\nWait, wait. Wait. That’s an aha moment I can flag here.\\nLet’s reevaluate this step-by-step to identify if the correct sum can be · · ·\\nWe started with the equation:\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥\\nFirst, let’s square both sides:\\n𝑎−√\\n𝑎+ 𝑥= 𝑥2 =⇒√\\n𝑎+ 𝑥= 𝑎−𝑥2\\nNext, I could square both sides again, treating the equation: · · ·\\n. . .\\nTable 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The\\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\\nallowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero\\nAlthough DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.\\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9\\ninclude:\\n• Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\\nwe design a readable pattern that includes a summary at the end of each response and\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\\nresults.\\n• Potential: By carefully designing the pattern for cold-start data with human priors, we\\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\\na better way for reasoning models.\\n2.3.2. Reasoning-oriented Reinforcement Learning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\\non enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such\\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language\\nmixing, we introduce a language consistency reward during RL training, which is calculated\\nas the proportion of target language words in the CoT. Although ablation experiments show\\nthat such alignment results in a slight degradation in the model’s performance, this reward\\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\\nreasoning tasks and the reward for language consistency by directly summing them to form the\\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\\non reasoning tasks.\\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\\nmodel’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.\\nReasoning data\\nWe curate reasoning prompts and generate reasoning trajectories by perform-\\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\\nwe expand the dataset by incorporating additional data, some of which use a generative reward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\\nabout 600k reasoning related training samples.\\n10\\nNon-Reasoning data\\nFor non-reasoning data, such as writing, factual QA, self-cognition,\\nand translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thought before answering the question by prompting. However, for simpler queries,\\nsuch as “hello” we do not provide a CoT in response. In the end, we collected a total of\\napproximately 200k training samples that are unrelated to reasoning.\\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\\n800k samples.\\n2.3.4. Reinforcement Learning for all Scenarios\\nTo further align the model with human preferences, we implement a secondary reinforcement\\nlearning stage aimed at improving the model’s helpfulness and harmlessness while simultane-\\nously refining its reasoning capabilities. Specifically, we train the model using a combination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\\nresponse of the model, including both the reasoning process and the summary, to identify and\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\\n2.4. Distillation: Empower Small Models with Reasoning Capability\\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that\\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\\nreasoning capability is slightly better than that of Llama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstage to the broader research community.\\n3. Experiment\\nBenchmarks\\nWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\\n11\\n2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese\\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\\nematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we\\nalso evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we\\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts\\nFollowing the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\\nevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\\nzero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\\nC#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated\\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\\nof 32,768 tokens for each benchmark.\\nBaselines\\nWe conduct comprehensive evaluations against several strong baselines, including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\nEvaluation Setup\\nWe set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-𝑝value of 0.95 to generate 𝑘\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\npass@1 = 1\\n𝑘\\n𝑘\\n∑︁\\n𝑖=1\\n𝑝𝑖,\\nwhere 𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n12\\n3.1. DeepSeek-R1 Evaluation\\nBenchmark (Metric)\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nSonnet-1022\\n0513\\nV3\\no1-mini o1-1217\\nR1\\nArchitecture\\n-\\n-\\nMoE\\n-\\n-\\nMoE\\n# Activated Params\\n-\\n-\\n37B\\n-\\n-\\n37B\\n# Total Params\\n-\\n-\\n671B\\n-\\n-\\n671B\\nEnglish\\nMMLU (Pass@1)\\n88.3\\n87.2\\n88.5\\n85.2\\n91.8\\n90.8\\nMMLU-Redux (EM)\\n88.9\\n88.0\\n89.1\\n86.7\\n-\\n92.9\\nMMLU-Pro (EM)\\n78.0\\n72.6\\n75.9\\n80.3\\n-\\n84.0\\nDROP (3-shot F1)\\n88.3\\n83.7\\n91.6\\n83.9\\n90.2\\n92.2\\nIF-Eval (Prompt Strict)\\n86.5\\n84.3\\n86.1\\n84.8\\n-\\n83.3\\nGPQA Diamond (Pass@1)\\n65.0\\n49.9\\n59.1\\n60.0\\n75.7\\n71.5\\nSimpleQA (Correct)\\n28.4\\n38.2\\n24.9\\n7.0\\n47.0\\n30.1\\nFRAMES (Acc.)\\n72.5\\n80.5\\n73.3\\n76.9\\n-\\n82.5\\nAlpacaEval2.0 (LC-winrate)\\n52.0\\n51.1\\n70.0\\n57.8\\n-\\n87.6\\nArenaHard (GPT-4-1106)\\n85.2\\n80.4\\n85.5\\n92.0\\n-\\n92.3\\nCode\\nLiveCodeBench (Pass@1-COT)\\n38.9\\n32.9\\n36.2\\n53.8\\n63.4\\n65.9\\nCodeforces (Percentile)\\n20.3\\n23.6\\n58.7\\n93.4\\n96.6\\n96.3\\nCodeforces (Rating)\\n717\\n759\\n1134\\n1820\\n2061\\n2029\\nSWE Verified (Resolved)\\n50.8\\n38.8\\n42.0\\n41.6\\n48.9\\n49.2\\nAider-Polyglot (Acc.)\\n45.3\\n16.0\\n49.6\\n32.9\\n61.7\\n53.3\\nMath\\nAIME 2024 (Pass@1)\\n16.0\\n9.3\\n39.2\\n63.6\\n79.2\\n79.8\\nMATH-500 (Pass@1)\\n78.3\\n74.6\\n90.2\\n90.0\\n96.4\\n97.3\\nCNMO 2024 (Pass@1)\\n13.1\\n10.8\\n43.2\\n67.6\\n-\\n78.8\\nChinese\\nCLUEWSC (EM)\\n85.4\\n87.9\\n90.9\\n89.9\\n-\\n92.8\\nC-Eval (EM)\\n76.7\\n76.0\\n86.5\\n68.9\\n-\\n91.8\\nC-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13\\nDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\\nits robustness across multiple tasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntraining data currently remains very limited.\\n3.2. Distilled Model Evaluation\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nGPT-4o-0513\\n9.3\\n13.4\\n74.6\\n49.9\\n32.9\\n759\\nClaude-3.5-Sonnet-1022\\n16.0\\n26.7\\n78.3\\n65.0\\n38.9\\n717\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nQwQ-32B-Preview\\n50.0\\n60.0\\n90.6\\n54.5\\n41.9\\n1316\\nDeepSeek-R1-Distill-Qwen-1.5B\\n28.9\\n52.7\\n83.9\\n33.8\\n16.9\\n954\\nDeepSeek-R1-Distill-Qwen-7B\\n55.5\\n83.3\\n92.8\\n49.1\\n37.6\\n1189\\nDeepSeek-R1-Distill-Qwen-14B\\n69.7\\n80.0\\n93.9\\n59.1\\n53.1\\n1481\\nDeepSeek-R1-Distill-Qwen-32B\\n72.6\\n83.3\\n94.3\\n62.1\\n57.2\\n1691\\nDeepSeek-R1-Distill-Llama-8B\\n50.4\\n80.0\\n89.1\\n49.0\\n39.6\\n1205\\nDeepSeek-R1-Distill-Llama-70B\\n70.0\\n86.7\\n94.5\\n65.2\\n57.5\\n1633\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\nAs shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v.s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?\\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\\n14\\nModel\\nAIME 2024\\nMATH-500\\nGPQA Diamond\\nLiveCodeBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nQwQ-32B-Preview\\n50.0\\n60.0\\n90.6\\n54.5\\n41.9\\nDeepSeek-R1-Zero-Qwen-32B\\n47.0\\n60.0\\n91.6\\n55.0\\n40.2\\nDeepSeek-R1-Distill-Qwen-32B\\n72.6\\n83.3\\n94.3\\n62.1\\n57.2\\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\\nscale reinforcement learning.\\n4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM)\\nPRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\nMonte Carlo Tree Search (MCTS)\\nInspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\nand the value model, iteratively refining the process.\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n15\\nexponentially larger search space. To address this, we set a maximum extension limit for each\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\nmodel to iteratively improve. While AlphaGo’s core success relied on training a value model to\\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\\ndue to the complexities of token generation.\\nIn conclusion, while MCTS can improve performance during inference when paired with a\\npre-trained value model, iteratively boosting model performance through self-search remains a\\nsignificant challenge.\\n5. Conclusion, Limitations, and Future Work\\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntuned models based on the same underlying checkpoints.\\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\\n• General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\\nthese fields.\\n• Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\\nin a language other than English or Chinese. We aim to address this limitation in future\\nupdates.\\n• Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive\\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shot setting for optimal results.\\n• Software Engineering Tasks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16\\nReferences\\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\\n-5-sonnet.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179.\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL\\nhttps://arxiv.org/abs/2210.10760.\\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or\\ng/10.48550/arXiv.2406.04127.\\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024.\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\\n17\\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941.\\nA. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA.\\nAmerican invitational mathematics examination - aime.\\nIn American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime.\\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\\ng-to-reason-with-llms/.\\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\\n-verified/.\\nQwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm\\n.github.io/blog/qwq-32b-preview/.\\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b\\nlog/qwen2.5.\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\\nGPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and\\nshogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,\\n2017a. URL http://arxiv.org/abs/1712.01815.\\n18\\nD. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\\nM. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and\\nD. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354–359,\\n2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033\\n14.\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv\\npreprint arXiv:2211.14275, 2022.\\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-\\nfree step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,\\n2023.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022.\\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\\nURL https://doi.org/10.48550/arXiv.2406.01574.\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang.\\nAgentless: Demystifying llm-based software\\nengineering agents. arXiv preprint, 2024.\\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,\\nQ. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing\\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL\\nhttps://arxiv.org/abs/2408.08152.\\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\\nevaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\\n19\\nAppendix\\nA. Contributions and Acknowledgments\\nCore Contributors\\nDaya Guo\\nDejian Yang\\nHaowei Zhang\\nJunxiao Song\\nRuoyu Zhang\\nRunxin Xu\\nQihao Zhu\\nShirong Ma\\nPeiyi Wang\\nXiao Bi\\nXiaokang Zhang\\nXingkai Yu\\nYu Wu\\nZ.F. Wu\\nZhibin Gou\\nZhihong Shao\\nZhuoshu Li\\nZiyi Gao\\nContributors\\nAixin Liu\\nBing Xue\\nBingxuan Wang\\nBochao Wu\\nBei Feng\\nChengda Lu\\nChenggang Zhao\\nChengqi Deng\\nChong Ruan\\nDamai Dai\\nDeli Chen\\nDongjie Ji\\nErhang Li\\nFangyun Lin\\nFucong Dai\\nFuli Luo*\\nGuangbo Hao\\nGuanting Chen\\nGuowei Li\\nH. Zhang\\nHanwei Xu\\nHonghui Ding\\nHuazuo Gao\\nHui Qu\\nHui Li\\nJianzhong Guo\\nJiashi Li\\nJingchang Chen\\nJingyang Yuan\\nJinhao Tu\\nJunjie Qiu\\nJunlong Li\\nJ.L. Cai\\nJiaqi Ni\\nJian Liang\\nJin Chen\\nKai Dong\\nKai Hu*\\nKaichao You\\nKaige Gao\\nKang Guan\\nKexin Huang\\nKuai Yu\\nLean Wang\\nLecong Zhang\\nLiang Zhao\\nLitong Wang\\nLiyue Zhang\\nLei Xu\\nLeyi Xia\\nMingchuan Zhang\\nMinghua Zhang\\nMinghui Tang\\nMingxu Zhou\\nMeng Li\\nMiaojun Wang\\nMingming Li\\nNing Tian\\nPanpan Huang\\nPeng Zhang\\nQiancheng Wang\\nQinyu Chen\\nQiushi Du\\nRuiqi Ge*\\nRuisong Zhang\\nRuizhe Pan\\nRunji Wang\\nR.J. Chen\\nR.L. Jin\\n20\\nRuyi Chen\\nShanghao Lu\\nShangyan Zhou\\nShanhuang Chen\\nShengfeng Ye\\nShiyu Wang\\nShuiping Yu\\nShunfeng Zhou\\nShuting Pan\\nS.S. Li\\nShuang Zhou\\nShaoqing Wu\\nShengfeng Ye\\nTao Yun\\nTian Pei\\nTianyu Sun\\nT. Wang\\nWangding Zeng\\nWen Liu\\nWenfeng Liang\\nWenjun Gao\\nWenqin Yu*\\nWentao Zhang\\nW.L. Xiao\\nWei An\\nXiaodong Liu\\nXiaohan Wang\\nXiaokang Chen\\nXiaotao Nie\\nXin Cheng\\nXin Liu\\nXin Xie\\nXingchao Liu\\nXinyu Yang\\nXinyuan Li\\nXuecheng Su\\nXuheng Lin\\nX.Q. Li\\nXiangyue Jin\\nXiaojin Shen\\nXiaosha Chen\\nXiaowen Sun\\nXiaoxiang Wang\\nXinnan Song\\nXinyi Zhou\\nXianzu Wang\\nXinxia Shan\\nY.K. Li\\nY.Q. Wang\\nY.X. Wei\\nYang Zhang\\nYanhong Xu\\nYao Li\\nYao Zhao\\nYaofeng Sun\\nYaohui Wang\\nYi Yu\\nYichao Zhang\\nYifan Shi\\nYiliang Xiong\\nYing He\\nYishi Piao\\nYisong Wang\\nYixuan Tan\\nYiyang Ma*\\nYiyuan Liu\\nYongqiang Guo\\nYuan Ou\\nYuduan Wang\\nYue Gong\\nYuheng Zou\\nYujia He\\nYunfan Xiong\\nYuxiang Luo\\nYuxiang You\\nYuxuan Liu\\nYuyang Zhou\\nY.X. Zhu\\nYanping Huang\\nYaohui Li\\nYi Zheng\\nYuchen Zhu\\nYunxian Ma\\nYing Tang\\nYukun Zha\\nYuting Yan\\nZ.Z. Ren\\nZehui Ren\\nZhangli Sha\\nZhe Fu\\nZhean Xu\\nZhenda Xie\\nZhengyan Zhang\\nZhewen Hao\\nZhicheng Ma\\nZhigang Yan\\nZhiyu Wu\\nZihui Gu\\n21\\nZijia Zhu\\nZijun Liu*\\nZilin Li\\nZiwei Xie\\nZiyang Song\\nZizheng Pan\\nZhen Huang\\nZhipeng Xu\\nZhongyu Zhang\\nZhen Zhang\\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\\ndenote individuals who have departed from our team.\\n22\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "\n",
    "docs = ArxivLoader(query=\"2501.12948\", load_max_docs=2).load()\n",
    "print(len(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Based Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://muhid-qaiser.medium.com/decoding-the-transformer-architecture-a-complete-guide-to-the-backbone-of-modern-ai-e554db9fe0c9', 'title': 'Decoding the Transformer Architecture: A Complete Guide to the Backbone of Modern AI | by Muhid Qaiser | Medium', 'description': 'The Transformer architecture has undoubtedly been a transformative breakthrough in AI. From natural language processing to computer vision and multimodal systems, the Transformer has become…', 'language': 'en'}, page_content=\"Decoding the Transformer Architecture: A Complete Guide to the Backbone of Modern AI | by Muhid Qaiser | MediumOpen in appSign upSign inWriteSign upSign inHomeLibraryStoriesStatsDecoding the Transformer Architecture: A Complete Guide to the Backbone of Modern AIMuhid Qaiser·Follow24 min read·Nov 12, 2024--ListenShareThe Transformer architecture has undoubtedly been a transformative breakthrough in AI. From natural language processing to computer vision and multimodal systems, the Transformer has become foundational to advancements across these fields.But what exactly makes the Transformer so powerful? Why does it excel where previous models struggled, and what unique challenges does it address? This article will dive into these questions, exploring the innovations behind the Transformer and why it’s such a game-changer.What is the Transformer?The Transformer model, introduced in the influential 2017 paper “Attention is All You Need” by Vaswani et al., was originally designed for machine translation. However, its flexible, robust architecture has since revolutionized modern AI, especially in tasks involving sequential data like text and images. The Transformer overcame the limitations of earlier models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), by leveraging a powerful mechanism called “self-attention” that allows it to capture relationships across long sequences efficiently and effectively.Fig 1: Architecture of TransformerLimitations of RNN and LSTMs:Before the transformer model was invented, sequence transduction models dominated the industry when it came to tasks that involved sequence such as machine translation. These models are usually based on complex RNN or LSTM that include an encoder and a decoder. The encoder would encode the given text into an encoded vector and the decoder would decode the encoded vector in the other language. These models had a great drawback, there were sequential and were computationally very expensive.1. Vanishing/Exploding GradientsTechnical Explanation: RNNs rely on backpropagation through time (BPTT) to update weights based on error gradients propagated through each time step. In practice, this approach causes gradients to diminish (vanish) or amplify (explode) as they move across long sequences, especially with deeper networks. This is due to repetitive multiplications of derivatives in each time step. In simple terms, the gradients become too small to make meaningful updates (vanishing), or too large, causing instability (exploding).LSTMs use gating mechanisms to control the flow of information and partially address vanishing gradients by allowing information to “skip” certain steps. However, they don’t completely solve the issue for very long sequences, as even LSTMs are still susceptible to vanishing gradients over very long contexts.Practical Example: In language modeling, consider a sentence: “The cat sat on the __.” A basic RNN will struggle to retain information about “cat” if there’s a long sequence of words between it and the blank. Due to vanishing gradients, the RNN essentially “forgets” the earlier part of the sequence, causing the model to fail in predicting contextually relevant words. LSTMs partially address this with gates (input, output, and forget gates) that help regulate the flow of information over time steps. However, with longer texts or sequences, even LSTMs still face limitations.2. Sequential Processing and Slow TrainingTechnical Explanation: RNNs and LSTMs process input data step-by-step. Each step depends on the computation of the previous one, which makes parallel processing difficult. This dependency results in longer training times and higher computational costs, especially when working with long sequences. Although models like bidirectional LSTMs allow processing in two directions, they don’t achieve full parallelism.Practical Example: In real-time speech recognition, where long audio sequences need to be processed quickly, RNNs and LSTMs take longer to train and produce output due to their sequential nature. Processing lengthy audio or video data also becomes inefficient, as each time step must complete before the next one can begin.3. Limited Memory for Long-Term DependenciesTechnical Explanation: Even with memory cells, LSTMs tend to “forget” information over long sequences. The gates in LSTMs only partially help in retaining relevant information but still prioritize more recent data over distant dependencies. Consequently, when models require deep context — such as understanding references in lengthy documents — RNNs and LSTMs struggle to retain and leverage information from earlier parts of the sequence.Practical Example: In machine translation, translating a long paragraph from one language to another often requires the model to remember information from the beginning of the paragraph to accurately translate later sentences. With RNNs or LSTMs, the translation quality diminishes as the model loses track of earlier parts, which is especially problematic for languages with complex sentence structures or that place crucial information at the start of sentences.Fig 2: Working of Recurrent Neural Networks (RNN)How Transformers Address These Limitations1. Self-Attention MechanismTechnical Explanation: The Transformer architecture replaces the sequential processing of RNNs with self-attention, which calculates the importance of each word or token relative to every other token in the sequence. Self-attention computes three vectors for each token: the query, key, and value vectors. For each token, the query vector is compared to every key vector, producing attention scores that determine how much focus to place on each other token. This approach enables each token to draw relevant contextual information from across the sequence, regardless of its position, effectively “remembering” long-range dependencies.Practical Example: In a sentence like “The scientist who won the Nobel Prize in 2019 was born in a small town,” self-attention allows the model to capture relationships across the sentence. Even if “scientist” and “small town” are far apart, the model can connect them by attending to all relevant parts of the sentence simultaneously, which RNNs would struggle to do.2. Parallelization and Faster TrainingTechnical Explanation: Unlike RNNs, which must process sequences step-by-step, the Transformer processes each token position independently by applying self-attention across the entire sequence at once. This parallelism enables Transformers to make full use of modern hardware, significantly accelerating training. Transformers also add positional encodings to the input data, which allows the model to retain information about the sequence order even when processing positions in parallel.Practical Example: In large-scale language modeling, such as training GPT-3, which has billions of parameters, parallel processing across thousands of tokens at once drastically reduces training time. Each word can be processed at the same time as all others, allowing models like BERT or GPT to be trained on massive datasets, spanning books and websites, in a reasonable amount of time. Training a similar-scale RNN would be practically infeasible due to slow sequential processing.3. Scalability and Capacity for Long-Range DependenciesTechnical Explanation: The Transformer’s multi-head self-attention extends its capacity to handle and process multiple dependencies across varying parts of the sequence. Multiple attention heads allow it to look at different parts of a sequence simultaneously and capture nuanced dependencies that might be overlooked if only one perspective was used. This makes Transformers highly effective at capturing complex relationships and long-term dependencies within long sequences.Practical Example: In document summarization, Transformers excel at identifying key points throughout an entire document without losing track of important details at the beginning. For instance, summarizing a legal document often requires understanding concepts introduced early on and connecting them to later clauses. Transformers handle this well, as their architecture allows them to attend to and relate information across the entire document, regardless of length.Fig 3: Comparison of Recurrent Neural Network and Transformer architectureNow that we are familiar with what the transformer model is, let’s dive into its architecture to see how it works!Architecture1. Encoder Decoder BlocksEncoder:· Multi-Head Self-Attention Block· Feed Forward NetworkFig 4 : Architecture of EncoderDecoder:· Masked Multi-Head Self-Attention Block· Multi-Head Self-Attention Block· Feed Forward NetworkFig 5 : Architecture of DecoderThey both contain a few similarities and differences which are mentioned below:Stack StructureBoth the Encoder and the Decoder are composed of a stack of N = 6 identical layersEach layer in the encoder has 2 sub-layers (multi-head self-attention and feed-forward network) which comprise an encoder layer.· Each layer in the decoder has three sub-layers: two of these are similar to the encoder’s sub-layers (multi-head self-attention and feed-forward network), but there is an additional third sub-layer (Masked Multi-Head Self-Attention Layer) in each decoder layer.· You can consider this as multiple blocks of encoder and decoder, where each output of the encoder gets fed into the next encoder until it reaches the final encoder, after which the output is fed into all decoders. Then the output of each decoder is passed onto the next decoder as input, along with the output of it’s corresponding encoder until the final decoder block is reached which passes it’s output into a Linear and Softmax layer to make the final prediction.Fig 6 : Working of Transfer of Attention Vector from Encoders to DecodersFig 7 : Transfer of Output from one Decoder to anotherMulti-Head AttentionOne of the common sub-layer between the encoder and the decoder is the multi-head attention mechanism sub-layer.Encoder: The encoder receives an input vector representing the data and outputs an attention vector, which emphasizes relevant pairs of words or tokens within the data, capturing relationships and dependencies across the entire input sequence.Decoder: The decoder takes the encoder’s output as input, allowing it to reference the entire encoded sequence and extract relevant context for generating coherent outputs. With its own self-attention mechanism, the decoder can also focus on previously generated tokens. This helps it align outputs with relevant input tokens during generation. For instance, in English-to-French translation, the encoder provides a context-rich representation of the English sentence, which the decoder uses to capture both local and long-range dependencies, ensuring accurate translation of each word.Fig 8 : Parallel Working of a Multi-Head Attention blockResidual Connections and Layer NormalizationResidual connections are used around each sub-layer. This technique helps in stabilizing the training of deep networks by ensuring gradient flow.Each sub-layer is followed by layer normalization, which normalizes the output of the sub-layer to help with convergence and reduce training time.This basically retains original information of the input even after it has been processed by each sub-layer.Fig 9 : Residual Connections involved in the Transformer ArchitectureMasked Self-Attention· The decoder’s self-attention sub-layer includes masking to ensure each position in the output sequence can only attend to positions before it or to itself, not to any future positions. This masking is essential for autoregressive generation, preventing the model from “cheating” by looking ahead.· As a result, each word/token in the sequence can only consider previous tokens, while future tokens contribute zero to the prediction. This enforces a strict left-to-right flow, allowing the model to generate outputs one token at a time without prematurely accessing future information.Fig 10 : What the model see before and after the maskingFig 11 : Scaled dot-product attention with Q-K dot product, masking to restrict focus, and softmax normalization.2. Embeddings LayerIn transformers, the embedding layer converts input tokens into dense vector representations that capture semantic information. These vectors serve as the starting point for processing by the encoder and decoder.In the decoder, self-attention is modified with masking to ensure each token can only attend to previous tokens or itself, not future tokens. This masking prevents “cheating,” enforcing an autoregressive, left-to-right generation where each token can only reference previous ones. Additionally, the output embeddings are offset by one position to ensure the decoder generates each token based solely on prior outputs.For example, during training, if the target output sequence isY=[y1,y2,y3,…,yn],the decoder’s input is offset to[<start>,y1,y2,…,yn−1].This offset guarantees that when predicting yi\\u200b, the decoder can only see tokens up to yi−1\\u200b, aligning with autoregressive sequence generation. For instance, while generating y3, the decoder only accesses [<start>,y1,y2] not y3\\u200b or any subsequent tokens like y4 or y5This offset mechanism helps in training consistency by making the learning conditions match inference conditions, where tokens are generated sequentially without future context. Through this setup, the model learns to depend solely on past tokens, ensuring coherent and context-aware output generation.Positional Encoding in TransformersIn Transformers, positional encodings are added to token embeddings to incorporate information about token positions. Unlike RNNs or CNNs, Transformers don’t process tokens sequentially, so they require positional encodings to understand sequence order.Why Positional Encoding Matters?Parallel processing in Transformers speeds up computation but leaves the model without an inherent sense of token order. Adding positional encodings to token embeddings enables the model to differentiate token positions.Formulation Using Sin and Cos FunctionsThe Transformer’s positional encoding formula is:PE(pos, 2i)=sin(pos / 10000^(2i/dmodel))PE(pos, 2i+1)=cos(pos / 10000^((2i)/dmodel))Here:pos is the token’s position in the sequence.i is the dimension index, and d_model is the dimensionality of the embeddings.This encoding uses sin for even dimensions and cos for odd, with frequencies scaled to create unique encodings per position. Lower dimensions oscillate slowly (capturing broader context), while higher ones oscillate faster (for finer positional nuances), this is due to the way the sine and cosine functions are scaled. The position pos is divided by a scaling factor 10000^(2i/dmodel), where i is the dimension index and dmodel is the total number of dimensions.For lower dimensions (small i), the scaling factor is smaller, which means the sine and cosine functions oscillate more slowly. Mathematically, this corresponds to larger wavelengths (slower oscillations).Example:For dmodel = 4:· For i = 0 (first dimension), the scaling factor is 10000^(0/4) = 1, meaning sin(pos) and cos(pos) will have slower oscillations.· For i = 1 (second dimension), the scaling factor is 10000^(1/4) ≈ 21.54, meaning sin(pos/21.54) and cos(pos/21.54) will oscillate faster.This slower oscillation in lower dimensions allows the model to capture broader positional context, like whether a token is near the beginning or end of a sequence.Key Benefits of Sine and Cosine Encoding· Unique Position Representation: The periodic nature of sine and cosine ensures unique, cyclic encodings, useful for identifying relative positions between tokens.· Relative Position Insight: This encoding lets the model calculate relative positions, as positional differences translate into functional shifts.· Extrapolation Ability: Periodicity allows the model to handle longer sequences than it saw during training.· Wavelength Scaling: Different frequencies across dimensions let the model address both local and long-range dependencies.Fig 12 : Periodic nature of the Sine and Cosine functionsExample CalculationConsider a 4-dimensional encoding for pos=1:PE(1,0) = sin(1) ≈ 0.841, PE(1,1) = cos(1) ≈ 0.540PE(1,2) ≈ 0.01, PE(1,3) ≈ 0.9998The encoding vector for pos=1 might look like [0.841, 0.540, 0.01, 0.9998].Practical ApplicationFor “The cat sat on the mat,” if “The” has an embedding of [0.2, 0.4, 0.5, 0.7] and position 0 encoding [0, 1, 0, 1], the final embedding for “The” would be [0.2, 1.4, 0.5, 1.7]. This combined vector now includes both semantic and positional information.Fig 13 : Calculations involed in find Positional Encodings of a given Sequence of tokens3. Position Wise Feed Forward Networks (FFN)The Position-wise Feed-Forward Network (FFN) is a key component in both the encoder and decoder of the Transformer architecture. It is used to process each word (or token) in the sequence independently, applying the same set of transformations at each position, but without influencing other positions. This helps the model learn complex, non-linear relationships at each token while maintaining the same parameters across the entire sequence.Where is it Used?The FFN is applied after the attention mechanism in both the encoder and decoder of the Transformer model. After the multi-head attention layer processes the input sequence, the output is passed through the position-wise FFN, which is applied to each position independently.Formula for FFNThe FFN applies two linear transformations with a ReLU activation in between to each token’s embedding, helping the model capture intricate patterns and relationships. The formula is:FFN(x) = max(0,xW1+b1)W2+b2Where:x is the input vector representing a word.W1 and W2\\u200b are learned weight matrices.b1 and b2 are bias vectors.ReLU is applied after the first linear transformation.Breakdown1. Input Vector (x):The input vector represents a word (or token) after attention processing, with dimensionality dmodel \\u200b (e.g., 512).Example: For the sentence “The cat sat on the mat,” each word like “The” and “cat” is represented by a vector of size 512.2. First Linear Transformation (xW1+b1\\u200b):The input vector xxx is multiplied by weight matrix W1\\u200b (dimensions: dmodel×df) with bias b1.This projects x into a higher-dimensional space (e.g., from 512 to 2048).3. ReLU Activation:ReLU replaces negative values with zero, adding non-linearity and enabling the model to learn complex relationships.4. Second Linear Transformation max(0,xW1+b1)W2+b2:The output from ReLU is multiplied by weight matrix W2 (dimensions: df×dmodel) and added to bias b2\\u200b, projecting the vector back to the original dimensionality (e.g., from 2048 back to 512).5. Output Vector:The final output vector has the same dimensionality as the input vector (dmodel=512) and is passed to the next layer.Practical ExampleFor the sentence “The cat sat on the mat”, the word “cat” is processed as follows:The 512-dimensional vector for “cat” is transformed using W1\\u200b and b1\\u200b (output size 2048), then ReLU is applied.The output is passed through W2\\u200b and b2\\u200b, resulting in a 512-dimensional output, which is the same size as the input.This process is repeated independently for each word in the sequence.Understanding “Two Convolutions with Kernel Size 1”The term “two convolutions with kernel size 1” is another way to describe the FFN. In traditional convolutions, a kernel moves across the sequence, processing neighboring words together. However, with kernel size 1, each word is processed independently, much like the FFN where each position is transformed independently with the same weights.In summary, the FFN applies two linear transformations to each word in the sequence independently, adding non-linearity through ReLU. This helps capture complex relationships at each position, while maintaining the same weight parameters across the entire sequence.4. Final Linear and Softmax LayerThe final linear layer and softmax layer in a Transformer model are responsible for generating the output predictions from the processed sequence. After the sequence has passed through the encoder-decoder architecture and the attention mechanisms, the model typically produces a vector of logits (raw values) for each token position.The linear layer applies a transformation that projects these logits into a space corresponding to the vocabulary size, effectively converting the model’s output into a set of potential token probabilities. Finally, the softmax layer is applied to these logits, normalizing them into a probability distribution, where each token’s probability represents the likelihood of it being the correct token at that position in the sequence. This step is crucial for tasks such as language modeling, machine translation, or text generation.Fig 14 : Example Architecture of Full Connected Linear Layer and a Softmax Layer with predictionsFig 15 : Example Architecture of Full Connected Linear LayerAttentionThe attention mechanism is central to the success of the transformer model, enabling it to overcome the limitations of sequential models like RNNs. It allows the model to focus on different parts of the input sequence simultaneously, thus providing a more flexible and efficient way of processing information.Self-AttentionComponents of Attention1. Query (Q): Represents what you are trying to focus on or understand in the input sequence. It’s like a question guiding the model’s attention to the most relevant data.Example: In the sentence “The cat sat on the mat,” the query might focus on understanding the action of the word “cat,” thus guiding the model to focus on words like “sat.”2. Key (K): Represents characteristics or features of each word in the input sequence, describing how relevant that word is in relation to the query.Example: For the word “sat,” its key might indicate its relevance in describing the action performed by the subject “cat.”3. Value (V): Contains the actual information or meaning associated with each word. These are the vectors that contribute to the final output.Example: The value for “sat” would contain detailed information about the action itself (e.g., its tense or context).Calculating Attention Weights1. Dot Product: The relevance of each key to the query is determined by calculating the dot product between the query and each key.Formula: Q⋅Ki \\u200b, which measures the similarity between the query and key.2. Scaling: The dot product scores are scaled by dividing by sqrt(dk)\\u200b\\u200b, where dk is the dimension of the key vectors. This helps prevent excessively large values, which could hinder the training process.Formula: Scaled Score = Q⋅Ki \\\\ sqrt(dk)3. Softmax: The scaled scores are passed through the softmax function, converting them into a probability distribution. This determines how much attention each word (value) should receive.Formula: αi = softmax( Q⋅Ki \\\\ sqrt(dk) )4. is a weighted sum of the value vectors, where each weight reflects the relevance of the corresponding key to the query.Formula: Output = ∑αi⋅ViFig 16 : Inner Working of the Attention MechanismFig 17 : Detailed Inner Working of the Attention MechanismMeaning of Weights on the Values· The attention weights αi determine how much importance each value Vi should have, based on the relationship between the query Q and the keys Ki\\u200b. If a key Ki closely matches the query Q, the corresponding weight αi is higher, emphasizing the value Vi.· For example, when understanding the action of “the cat,” if the word “sat” has a higher weight, it indicates that “sat” is more relevant in answering the query about the cat’s action. The values Vi\\u200b are multiplied by their respective attention weights αi, so relevant words with higher weights contribute more to the final output, while less relevant words have a lesser impact.· In practice, when summarizing based on the query “cat,” words like “sat” will influence the summary more than words like “on” or “the.The value vector is not just a simple representation of the word; it is enriched with various features that capture the meaning of the word as well as its role within the sentence. Here’s what that could include for “sat”:· Word Meaning: “Sat” is the past tense of “sit.”· Tense and Grammar: Indicates past tense, differentiating it from “sit.”· Position and Syntax: Shows that “sat” follows “cat” and serves as the verb.· Role in Context: Indicates “sat” is the action performed by “cat.”· Contextual Features: Indicates that “sat” describes a completed action.Fig 18 : A simplified architecture of the Attention Block.Multi Head AttentionIn single-head attention, the model learns only one set of attention weights, limiting its ability to capture diverse relationships between words. Multi-head attention improves on this by using multiple “heads,” each learning its own set of weights. This enables the model to focus on different aspects of the sentence simultaneously, such as syntax, semantics, or context.ExampleIn the sentence “The dog chased the ball, and the cat watched,” different heads might focus on:Syntactic Structure: Recognizing subject-object relationships, like “dog-chased-ball.”Semantic Focus: Identifying related entities, like “dog” and “cat.”Contextual Meaning: Capturing the cause-effect relationship between actions, like “chasing” vs. “watching.”Positional Context: A head focusing on positional dependencies, ensuring “sat” comes after “cat.”Each attention head operates in a distinct subspace, allowing the model to capture multiple perspectives of the data, leading to richer, more nuanced representations than single-head attention, which is especially beneficial for tasks requiring complex contextual understanding.Representation Subspaces and PositionsSubspaces: Each “head” in multi-head attention learns a separate, distinct way to represent the input data. These subspaces are different projections of the original word representations (embeddings) and capture different types of relationships or patterns in the data. For instance, one head might focus on syntax, another on semantics, and another on word order.Positions: Different heads can also focus on different positions in the input. Some heads capture local dependencies, while others focus on long-range dependencies. For instance, one head might focus on the relationship between “cat” and “sat” in a sentence, while another might capture the relationship between “cat” and “mat”, or even between distant words like “sat” and “on”.ParallelizationMulti-head attention allows the model to compute attention for all words simultaneously, reducing computation time and increasing efficiency. Each head processes the sentence independently, looking at different aspects like syntactic patterns or semantic relationships.This approach ensures that the model can simultaneously attend to both short-range and long-range relationships, allowing it to understand complex patterns and dependencies across the entire sentence.Fig 19 : The flow of Q, K and V keys throughout the Attention BlockApplication of Attention Mechanism in TransformerIn the encoder-decoder attention mechanism of a transformer, the queries come from the decoder, while the keys and values come from the encoder output. This setup allows the decoder to access all the relevant information from the input sequence during the generation process. Here’s how it works:Steps in Encoder-Decoder Attention1. Encoder Output:The encoder processes the input sequence (e.g., “The cat sat on the mat”) and generates contextualized representations for each token.These outputs (or memory) serve as the keys and values used in the attention mechanism. Where the output values are attention vectors.2. Decoder Query:The decoder generates the output sequence, predicting the next token based on the tokens generated so far.The query comes from the previous decoder layer or step and reflects the decoder’s current focus.3. Attention Mechanism:The decoder uses the query to interact with the encoder’s output (keys and values).The attention score is computed between the query and keys to decide which input tokens are most relevant.The values (representing the content of the tokens) are weighted by these attention scores and summed to form the input for the next decoder layer.Why Only Keys and Values Come from the Encoder?Query (Decoder): Represents what the decoder needs based on the previously generated tokens.Keys and Values (Encoder): Represent the context of the input sequence, providing information the decoder uses to generate the output.ExampleWhen predicting the word “sat” in “The cat sat on the mat”:· When predicting the word “sat” (at position 3), The query vector represents the current state of the decoder (e.g., “The cat”).· The encoder’s output (which contains context for the entire input sentence, “The cat sat on the mat”) is used as the keys and values.· The decoder uses the query to attend to these encoder outputs and previous states to predict “sat.”· At the time of predicting “sat,” the decoder focuses on the query vector formed by “The” and “cat,” which leads the attention mechanism to assign high weights to the word “cat” (since it’s relevant for the action) and “sat” (to understand the verb’s relationship with the subject).· Note: Even though “sat” hasn’t been predicted yet, the model can still attend to it because the encoder has a rich contextual representation of the entire input, and “sat” is likely weighted for its action-related relevance.This structure allows the decoder to consider the entire input sequence for generating each token in the output sequence, making it ideal for tasks like machine translation, where understanding the full context of the input is crucial.Fig 20: Flow of information in the all of the Encoder and Decoder blocksLong-Short Range DependenciesIn sequence-based models like transformers, learning long-range dependencies is crucial for tasks where distant elements in the input or output are related. However, understanding how information flows through layers is essential for capturing these dependencies. The passage of information, or path length, directly influences the model’s ability to learn complex relationships across distant positions in a sequence.Long-Range DependenciesLong-range dependencies refer to the relationships between distant elements in a sequence. For example, in a sentence, the word “cat” may be related to the word “mat,” which appears much later in the sentence. For a model to predict “mat” based on “cat,” it needs to learn the long-range dependency between these two words.Path LengthsPath length refers to the number of layers or steps that the signal (information) must traverse to reach another part of the sequence. In neural networks, the shorter the path, the quicker and easier it is for the model to learn dependencies between different parts of the input and output sequences.Forward and Backward SignalsForward signals refer to the flow of information from the input sequence to the output sequence, while backward signals involve information flowing in reverse, from the output back to the input. In networks that process sequences (e.g., transformers, RNNs), the signals travel through multiple layers to pass information between different positions in the input or output. The length of the path a signal travels is critical for learning long-range dependencies.Challenges of Long-Range DependenciesLearning long-range dependencies becomes difficult when path lengths are long, as the information can become diluted or distorted through many layers. Additionally, gradients in backpropagation can either vanish or explode, making learning ineffective over long distances.Shorter Path Lengths = Easier LearningThe shorter the path length between any two positions in the input and output sequences, the easier it is to learn the relationship between those positions.In the transformer’s architecture, the self-attention mechanism helps reduce the effective path length by allowing any input position to directly attend to any other position, thus shortening the path compared to models like RNNs, where signals need to pass through many sequential layers. This direct access to other positions is achieved in constant time, O(1), as opposed to the sequential information flow in RNNs O(n). Maximum Path Length metrix is used for this purposeThe maximum path length is a key metric that helps measure how far signals have to travel between any two positions in the sequence.Layer Types and Path LengthDifferent types of layers in a network, such as convolutional layers, recurrent layers, or transformer layers, impact the path length in different ways:Convolutional layers typically have fixed-length receptive fields, meaning they can look at a fixed range of neighboring elements. O(log_k(n))Recurrent layers process the sequence step by step, which means that the signal must pass through many steps (or layers) to reach distant parts of the sequence. O(n)Transformer layers use self-attention, allowing direct connections between any positions in the sequence, reducing the effective path length. O(1)Fig 21 : Shows how the model captures both Long and Short ranged dependenciesWeak Inductive BiasWhat is Inductive Bias?Inductive bias refers to the set of assumptions a model makes to generalize from the training data to unseen data. This bias influences how effectively a model can learn specific tasks. Different models have different inductive biases based on their architecture and how they process information.Strong inductive bias means the model has more built-in assumptions, making it better suited for certain tasks but potentially less flexible for others.Weak inductive bias means the model has fewer built-in assumptions, making it more flexible and adaptable but requiring more data and training to learn effectively.Practical Example of Inductive BiasImagine you are training a model to recognize objects in an image:Model A (Strong Inductive Bias): This model has built-in assumptions that images have local patterns that repeat (e.g., edges, textures). It is specifically designed to focus on small regions of the image at a time. A convolutional neural network (CNN) is a good example of this model.Model B (Weak Inductive Bias): This model has fewer assumptions about the structure of the data. It can learn any kind of relationship from scratch but needs more data to do so. A transformer model fits into this category.How Inductive Bias Relates to CNNs and Transformers1. CNNs (Strong Inductive Bias):Built-in Assumptions: CNNs assume that the data has spatial locality. They are specifically designed to focus on local features in images (e.g., detecting edges, corners).How It Works: A CNN scans small sections of the image with convolutional filters, learning to detect patterns such as lines or shapes. This makes CNNs highly effective for tasks where local patterns matter, such as image classification or object detection.Example: When recognizing a cat in an image, a CNN might first detect the cat’s ears, eyes, and fur texture, then piece together these local patterns to understand the overall picture.2. Transformers (Weak Inductive Bias):Built-in Assumptions: Transformers have minimal built-in assumptions about the structure of the data. They use self-attention mechanisms to learn relationships between all parts of the input data simultaneously, regardless of their position or locality.How It Works: A transformer processes the entire input sequence at once, considering each part in relation to every other part. This flexibility allows it to learn complex relationships but requires more data and computational power.Example: If a transformer is used for image recognition, it can learn to understand the whole image holistically by attending to all parts at once. This means it could identify a cat by considering the entire shape and context of the image without being limited to local regions.Comparison Between CNNs and Transformers1. Local vs. Global Understanding:CNNs focus on local patterns and progressively build up an understanding of the image. This inductive bias helps them excel in tasks where local features are critical.Transformers, on the other hand, can learn global relationships from the start, considering the entire input at once. They do not assume that the data has local structures, making them more flexible but requiring more data and training.2. Efficiency vs. Flexibility:CNNs are more efficient for tasks that align with their inductive bias, such as image processing, where local features are essential.Transformers are more flexible and can learn a wider range of relationships. This flexibility means they can be applied to various tasks, such as language modeling, image classification, or any domain where data may not have clear local structures.Practical ImplicationSuppose you have an image recognition task:A CNN would quickly learn to recognize the cat by focusing on its ears and eyes due to its strong inductive bias for spatial locality.A transformer might take longer and require more data to achieve the same level of accuracy because it needs to learn from scratch that certain features (e.g., ears and eyes) are important for recognizing a cat. However, once trained, it could potentially learn more complex relationships, such as the overall context or interaction between different parts of the image.Fig 22 : The model's learning process, extracting information from image patches without strong inductive biases.ConclusionIn conclusion, the Transformer has fundamentally reshaped the landscape of AI by introducing an architecture that is both powerful and versatile. Through self-attention, multi-headed attention, and position-wise feed-forward layers, the Transformer overcomes the challenges of previous models, enabling more efficient and effective handling of long-range dependencies in sequential data. Its design not only supports complex, nuanced representations but also scales impressively, unlocking advancements in natural language processing, computer vision, and beyond. As researchers continue to build on this foundation, extending the Transformer into new modalities and applications, it remains clear that this architecture will continue to drive innovation in AI, opening doors to even more sophisticated and capable systems.BibliographyAttention is All You Need research paperhttps://www.linkedin.com/feed/update/urn:li:activity:7260914752212598784/TransformersAttentionAINLPGenerative Ai Tools----FollowWritten by Muhid Qaiser3 Followers·2 FollowingA passionate Aritificial Intelligence student, dedicate in the learning the craft of Natural Language Processing and Computer VisionFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://muhid-qaiser.medium.com/decoding-the-transformer-architecture-a-complete-guide-to-the-backbone-of-modern-ai-e554db9fe0c9\",),\n",
    "              bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                #   class_=(\"post-title\", \"post-header\", \"post-content\")  # What Tags you want to extract information from using Bs4\n",
    "              )))\n",
    "    \n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Based Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'DeepSeek', 'summary': 'Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd., doing business as DeepSeek, is a Chinese artificial intelligence company that develops large language models (LLMs). Based in Hangzhou, Zhejiang, it is owned and funded by the Chinese hedge fund High-Flyer. DeepSeek was founded in July 2023 by Liang Wenfeng, the co-founder of High-Flyer, who also serves as the CEO for both companies. The company launched an eponymous chatbot alongside its DeepSeek-R1 model in January 2025.\\nReleased under the MIT License, DeepSeek-R1 provides responses comparable to other contemporary large language models, such as OpenAI\\'s GPT-4o and o1. Its training cost is reported to be significantly lower than other LLMs. The company claims that it trained its V3 model for US$6 million compared to $100 million for OpenAI\\'s GPT-4 in 2023, and approximately one-tenth of the computing power used for Meta\\'s comparable model, Llama 3.1. DeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\".\\nDeepSeek\\'s models are \"open weight\", which provides less freedom for modification than true open-source software. The company reportedly recruits AI researchers from top Chinese universities and hires from outside the computer science field to diversify its models\\' knowledge and abilities.\\nThe DeepSeek R1 model was trained at a significantly lower cost than other models by using techniques such as mixture of experts to reduce costs. The model was also trained during ongoing trade restrictions on AI chip exports to China, causing it to be trained on weaker AI chips made for export to China, and using fewer chips compared to other models. This breakthrough in reducing expenses, although increasing efficiency and maintaining the model\\'s performance power and quality in the AI industry, sent \"shockwaves\" through the market. It threatened the dominance of AI leaders like Nvidia and contributed to the largest drop for a single company in US stock market history, as Nvidia lost $600 billion in market value.', 'source': 'https://en.wikipedia.org/wiki/DeepSeek'}, page_content='Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd., doing business as DeepSeek, is a Chinese artificial intelligence company that develops large language models (LLMs). Based in Hangzhou, Zhejiang, it is owned and funded by the Chinese hedge fund High-Flyer. DeepSeek was founded in July 2023 by Liang Wenfeng, the co-founder of High-Flyer, who also serves as the CEO for both companies. The company launched an eponymous chatbot alongside its DeepSeek-R1 model in January 2025.\\nReleased under the MIT License, DeepSeek-R1 provides responses comparable to other contemporary large language models, such as OpenAI\\'s GPT-4o and o1. Its training cost is reported to be significantly lower than other LLMs. The company claims that it trained its V3 model for US$6 million compared to $100 million for OpenAI\\'s GPT-4 in 2023, and approximately one-tenth of the computing power used for Meta\\'s comparable model, Llama 3.1. DeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\".\\nDeepSeek\\'s models are \"open weight\", which provides less freedom for modification than true open-source software. The company reportedly recruits AI researchers from top Chinese universities and hires from outside the computer science field to diversify its models\\' knowledge and abilities.\\nThe DeepSeek R1 model was trained at a significantly lower cost than other models by using techniques such as mixture of experts to reduce costs. The model was also trained during ongoing trade restrictions on AI chip exports to China, causing it to be trained on weaker AI chips made for export to China, and using fewer chips compared to other models. This breakthrough in reducing expenses, although increasing efficiency and maintaining the model\\'s performance power and quality in the AI industry, sent \"shockwaves\" through the market. It threatened the dominance of AI leaders like Nvidia and contributed to the largest drop for a single company in US stock market history, as Nvidia lost $600 billion in market value.\\n\\n\\n== History ==\\n\\n\\n=== Founding and early years (2016–2023) ===\\nIn February 2016, High-Flyer was co-founded by AI enthusiast Liang Wenfeng, who had been trading since the 2007–2008 financial crisis while attending Zhejiang University. The company began stock trading using a GPU-dependent deep learning model on 21 October 2016. Prior to this, it used CPU-based models, mainly linear models. Most trading was driven by AI by the end of 2017.\\nIn 2019, Liang established High-Flyer as a hedge fund focused on developing and using AI trading algorithms. By 2021, High-Flyer exclusively used AI in trading, often using Nvidia chips.\\nInitial computing cluster Fire-Flyer began construction in 2019 and finished in 2020, at a cost of 200 million yuan. It contained 1,100 GPUs interconnected at a rate of 200 Gbit/s. It was \\'retired\\' after 1.5 years in operation.\\nIn 2021, Liang began stockpiling Nvidia GPUs for an AI project. According to 36Kr, Liang acquired 10,000 Nvidia A100 GPUs before the United States restricted chip sales to China. Computing cluster Fire-Flyer 2 began construction in 2021 with a budget of 1 billion yuan.\\nIt was reported that in 2022, Fire-Flyer 2\\'s capacity had been used at over 96%, totaling 56.74 million GPU hours. 27% was used to support scientific computing outside the company.\\nDuring 2022, Fire-Flyer 2 had 5000 PCIe A100 GPUs in 625 nodes, each containing 8 GPUs. At the time, it exclusively used PCIe instead of the DGX version of A100, since at the time the models it trained could fit within a single 40 GB GPU VRAM and so there was no need for the higher bandwidth of DGX (i.e., it required only data parallelism but not model parallelism). Later, it incorporated NVLinks and NCCL to train larger models that required model parallelism.\\nOn 14 April 2023, High-Flyer announced the start of an artificial general intelligence lab dedicated to research developing AI tools separate from High-Flyer\\'s financial'),\n",
       " Document(metadata={'title': 'DeepSeek (chatbot)', 'summary': 'DeepSeek is a chatbot created by the Chinese artificial intelligence company DeepSeek.\\nReleased on 10 January 2025, DeepSeek-R1 surpassed ChatGPT as the most downloaded freeware app on the iOS App Store in the United States by 27 January. DeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\" and initiating \"a global AI space race\". DeepSeek\\'s compliance with Chinese government censorship policies and its data collection practices have also raised concerns over privacy and information control in the model, prompting regulatory scrutiny in multiple countries.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/DeepSeek_(chatbot)'}, page_content='DeepSeek is a chatbot created by the Chinese artificial intelligence company DeepSeek.\\nReleased on 10 January 2025, DeepSeek-R1 surpassed ChatGPT as the most downloaded freeware app on the iOS App Store in the United States by 27 January. DeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\" and initiating \"a global AI space race\". DeepSeek\\'s compliance with Chinese government censorship policies and its data collection practices have also raised concerns over privacy and information control in the model, prompting regulatory scrutiny in multiple countries.\\n\\n\\n== History ==\\nOn 10 January 2025, DeepSeek released the chatbot, based on the DeepSeek-R1 model, for iOS and Android. By 27 January, DeepSeek-R1 surpassed ChatGPT as the most-downloaded freeware app on the iOS App Store in the United States, which resulted in an 18% drop in Nvidia’s share price. After a \"large-scale\" cyberattack on the same day disrupted the proper functioning of its servers, DeepSeek limited its new user registration to phone numbers from mainland China, email addresses, or Google account logins.\\n\\n\\n== Usage ==\\nDeepSeek can answer questions, solve logic problems, and write computer programs on par with other chatbots, according to benchmark tests used by American AI companies.\\n\\n\\n== Operation ==\\nDeepSeek-V3 uses significantly fewer resources compared to its peers. For example, whereas the world\\'s leading AI companies train their chatbots with supercomputers using as many as 16,000 graphics processing units (GPUs), DeepSeek claims to have needed only about 2,000 GPUs—namely, the H800 series chips from Nvidia. It was trained in around 55 days at a cost of US$5.58 million, which is roughly one-tenth of what tech giant Meta spent building its latest AI technology.\\n\\n\\n== Reactions ==\\nDeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\", constituting \"the first shot at what is emerging as a global AI space race\", and ushering in \"a new era of AI brinkmanship\".\\n\\n\\n=== Challenge to US AI dominance ===\\nDeepSeek\\'s competitive performance at relatively minimal cost has been recognized as potentially challenging the global dominance of American AI models. Various publications and news media, such as The Hill and The Guardian, have described the release of the R1 chatbot as a \"Sputnik moment\" for American AI, echoing Marc Andreessen\\'s view.\\n\\n\\n=== Chinese respect ===\\nDeepSeek\\'s founder Liang Wenfeng has been compared to OpenAI CEO Sam Altman, with CNN calling him the Sam Altman of China and an evangelist for AI. Chinese state media widely praised DeepSeek as a national asset. On 20 January 2025, China\\'s Premier Li Qiang invited Wenfeng to his symposium with experts and asked him to provide opinions and suggestions on a draft for comments of the annual 2024 government work report.\\n\\n\\n=== Performance and success ===\\nLeading figures in the American AI sector had mixed reactions to DeepSeek\\'s performance and success. Microsoft CEO Satya Nadella and Altman—whose companies are involved in the United States government-backed \"Stargate Project\" to develop American AI infrastructure—both called DeepSeek \"super impressive\". Various companies including Amazon Web Services, Toyota, and Stripe are seeking to use the model in their program. When American President Donald Trump announced The Stargate Project, he referred to DeepSeek as a wake-up call and a positive development. \\nOther leaders in the AI field, however—including Scale AI CEO Alexandr Wang, Anthropic cofounder and CEO Dario Amodei, and Elon Musk—have expressed skepticism of the app\\'s performance or of the sustainability of its success. \\n\\n\\n=== Stock market implications ===\\nDeepSeek\\'s optimization of limited resources has highlighted potential limits of United States sanctions on China\\'s AI development, including export restrictions on advanced AI chips to China. The success of the company\\'s AI models consequently \"sparked market turmoil\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"DeepSeek\", load_max_docs=2).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
